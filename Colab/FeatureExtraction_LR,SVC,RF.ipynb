{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16728,
     "status": "ok",
     "timestamp": 1681762201724,
     "user": {
      "displayName": "Bedri Keskin",
      "userId": "15100244906447267406"
     },
     "user_tz": -180
    },
    "id": "Lpr5fMmDdwrp",
    "outputId": "7f6c5777-3684-4fa5-ffbd-5dcfdb81a30a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qz/hql_2tx50xs57nqrwmzl71qw0000gn/T/ipykernel_57609/4027096586.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from google.colab import drive\n",
    "import joblib\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "path_Algol = '/content/gdrive/MyDrive/Doktora/Python/Algol'\n",
    "path_Algol_Noisy = '/content/gdrive/MyDrive/Doktora/Python/Algol_Noisy'\n",
    "Algol_files = [os.path.join(path_Algol, f) for f in os.listdir(path_Algol) if f.endswith('.txt')]\n",
    "Algol_files = Algol_files + [os.path.join(path_Algol_Noisy, f) for f in os.listdir(path_Algol_Noisy) if f.endswith('.txt')]\n",
    "data_Algol = []\n",
    "labels_Algol = []\n",
    "\n",
    "for file in Algol_files:\n",
    "  labels_Algol.append(\"Algol\")\n",
    "  fileContent = []\n",
    "  with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # Satırları ayrıştırın ve x, y koordinatlarını depolayın\n",
    "            x, y = line.strip().split('\\t')\n",
    "            fileContent.append(float(y))\n",
    "  data_Algol.append(fileContent)\n",
    "\n",
    "path_Beta_Lyrae = '/content/gdrive/MyDrive/Doktora/Python/Beta_Lyrae'\n",
    "path_Beta_Lyrae_Noisy = '/content/gdrive/MyDrive/Doktora/Python/Beta_Lyrae_Noisy'\n",
    "Beta_Lyrae_files = [os.path.join(path_Beta_Lyrae, f) for f in os.listdir(path_Beta_Lyrae) if f.endswith('.txt')]\n",
    "Beta_Lyrae_files = Beta_Lyrae_files + [os.path.join(path_Beta_Lyrae_Noisy, f) for f in os.listdir(path_Beta_Lyrae_Noisy) if f.endswith('.txt')]\n",
    "data_Beta_Lyrae = []\n",
    "labels_Beta_Lyrae = []\n",
    "\n",
    "for file in Beta_Lyrae_files:\n",
    "  labels_Beta_Lyrae.append(\"B Lyr\")\n",
    "  fileContent = []\n",
    "  with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # Satırları ayrıştırın ve x, y koordinatlarını depolayın\n",
    "            x, y = line.strip().split('\\t')\n",
    "            fileContent.append(float(y))\n",
    "  data_Beta_Lyrae.append(fileContent)\n",
    "\n",
    "path_W_UMa = '/content/gdrive/MyDrive/Doktora/Python/W_UMa_Noisy'\n",
    "path_W_UMa_Noisy = '/content/gdrive/MyDrive/Doktora/Python/W_UMa_Noisy'\n",
    "W_UMa_files = [os.path.join(path_W_UMa, f) for f in os.listdir(path_W_UMa) if f.endswith('.txt')]\n",
    "W_UMa_files = W_UMa_files + [os.path.join(path_W_UMa_Noisy, f) for f in os.listdir(path_W_UMa_Noisy) if f.endswith('.txt')]\n",
    "data_W_UMa = []\n",
    "labels_W_UMa = []\n",
    "\n",
    "for file in W_UMa_files:\n",
    "  labels_W_UMa.append(\"W UMa\")\n",
    "  fileContent = []\n",
    "  with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # Satırları ayrıştırın ve x, y koordinatlarını depolayın\n",
    "            x, y = line.strip().split('\\t')\n",
    "            fileContent.append(float(y))\n",
    "  data_W_UMa.append(fileContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1681762201727,
     "user": {
      "displayName": "Bedri Keskin",
      "userId": "15100244906447267406"
     },
     "user_tz": -180
    },
    "id": "Xsq82w9oeJ9O"
   },
   "outputs": [],
   "source": [
    "# Verileri ve etiketleri numpy dizilerine dönüştürün\n",
    "X = np.array(data_Algol+data_Beta_Lyrae+data_W_UMa)\n",
    "y = np.array(labels_Algol+labels_Beta_Lyrae+labels_W_UMa)\n",
    "\n",
    "X = X.astype('float')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2561445,
     "status": "ok",
     "timestamp": 1681774993471,
     "user": {
      "displayName": "Bedri Keskin",
      "userId": "15100244906447267406"
     },
     "user_tz": -180
    },
    "id": "wSO4mQ34hNqd",
    "outputId": "681a4603-2652-449a-975b-990d8e596c65"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MinMaxScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qz/hql_2tx50xs57nqrwmzl71qw0000gn/T/ipykernel_57609/4222646039.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# transforms for the feature union\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtransforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MinMaxScaler' is not defined"
     ]
    }
   ],
   "source": [
    "# transforms for the feature union\n",
    "transforms = list()\n",
    "transforms.append(('mms', MinMaxScaler()))\n",
    "transforms.append(('ss', StandardScaler()))\n",
    "transforms.append(('rs', RobustScaler()))\n",
    "transforms.append(('qt', QuantileTransformer(n_quantiles=100, output_distribution='normal')))\n",
    "transforms.append(('kbd', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')))\n",
    "transforms.append(('pca', PCA(n_components=7)))\n",
    "transforms.append(('svd', TruncatedSVD(n_components=7)))\n",
    "# create the feature union\n",
    "fu = FeatureUnion(transforms)\n",
    "\n",
    "# define the feature selection\n",
    "rfe = RFE(estimator=LogisticRegression(solver='liblinear'), n_features_to_select=15)\n",
    "\n",
    "# define the model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "#model = SVC(kernel='rbf', gamma='scale', C=1.0)\n",
    "#model = RandomForestClassifier()\n",
    "\n",
    "# define the pipeline\n",
    "steps = list()\n",
    "steps.append(('fu', fu))\n",
    "steps.append(('rfe', rfe))\n",
    "steps.append(('m', model))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# define the cross-validation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMuSLOBTcgdvXJ4ZXI2u0g9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
